# Project Settings
run_name: PKU_bert
model_name: bert
save_dir: ./output
seed: 42
wandb: false
wandb_entity: azerrroth
wandb_project: nlp-bilstm
warmup_steps: 10

# Training parameters
base_lr: 0.001
batch_size: 16
decay_factor: 0.5
early_stopping: true
init_lr: 1.0e-10
l2_coeff: 1.0e-06
gradient_clip_norm: 0

# Datasets
gold_data_path: seg-data/gold/pku_test_gold.utf8
test_data_path: seg-data/testing/pku_test.utf8
train_data_path: seg-data/training/pku_training.utf8
label2id:
  B: 0
  E: 2
  M: 1
  S: 3
label_size: 4
max_len: 128
num_workers: 16
limit_max_len: true
token_length: 512

# Model parameters
bert_model_name: bert-base-chinese
embedding_dim: 10
vocab_size: 10000000.0
hidden_dim: 256
lstm_dropout: 0.2
num_layers: 3
